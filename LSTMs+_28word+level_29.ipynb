{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from numpy import array\n",
    "from pickle import dump\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/Chanti'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/Chanti/Desktop/Cookbook/Chapter 8\n"
     ]
    }
   ],
   "source": [
    "cd '/Users/Chanti/Desktop/Cookbook/Chapter 8'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/Chanti/Desktop/Cookbook/Chapter 8'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load doc into memory\n",
    "def load_document(name):\n",
    "    file = open(name, 'r')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "﻿The Project Gutenberg EBook of The Jungle Book, by Rudyard Kipling\n",
      "\n",
      "This eBook is for the use of anyone anywhere at no cost and with\n",
      "almost no restrictions whatsoever.  You may copy it, give it away or\n",
      "re-use it under the terms of the Project Gutenberg License included\n",
      "with this eBook or online at www.gutenberg.org\n",
      "\n",
      "\n",
      "Title: The Jungle Book\n",
      "\n",
      "Author: Rudyard Kipling\n",
      "\n",
      "Release Date: January 16, 2006 [EBook #236]\n",
      "Last Updated: October 6, 2016\n",
      "\n",
      "Language: English\n",
      "\n",
      "Character set encoding: UTF-8\n",
      "\n",
      "*** START OF THIS PROJECT GUTENBERG EBOOK THE JUNGLE BOOK ***\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Produced by An Anonymous Volunteer and David Widger\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "THE JUNGLE BOOK\n",
      "\n",
      "By Rudyard Kipling\n",
      "\n",
      "\n",
      "\n",
      "Contents\n",
      "\n",
      "     Mowgli’s Brothers\n",
      "     Hunting-Song of the Seeonee Pack\n",
      "     Kaa’s Hunting\n",
      "     Road-Song of the Bandar-Log\n",
      "     “Tiger! Tiger!”\n",
      "      Mowgli’s Song\n",
      "     The White Seal\n",
      "     Lukannon\n",
      "     “Rikki-Tikki-Tavi”\n",
      "      Darzee’s Chant\n",
      "     Toomai of the Elephants\n",
      "     Shiv and the Grasshopper\n",
      "     Her Majesty’s Servants\n",
      "     Parade Song of the Camp Animals\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Mowgli’s Brothers\n",
      "\n",
      "     Now Rann the Kite brings home the night\n",
      "        That Mang the Bat sets free--\n",
      "     The herds are shut in byre and hut\n",
      "        For loosed till dawn are we.\n",
      "     This is the hour of pride and power,\n",
      "        Talon and tush and claw.\n",
      "     Oh, hear the call!--Good hunting all\n",
      "        That keep the Jungle Law!\n",
      "     Night-Song in the Jungle\n",
      "\n",
      "It was seven o’clock of a very warm evening in the Seeonee hills when\n",
      "Father Wolf woke up from his day’s rest, scratched himself, yawned, and\n",
      "spread out his paws one after the other to get rid of the sleepy feeling\n",
      "in their tips. Mother Wolf lay with her big gray nose dropped across her\n",
      "four tumbling, squealing cubs, and the moon shone into the mouth of the\n",
      "cave where they all lived. “Augrh!” said Father Wolf. “It is time to\n",
      "hunt again.” He was going to spring down hill when a little shadow with\n",
      "a bushy tail crossed the threshold and whined: “Good luck go with you, O\n",
      "Chief of the Wolves. And good luck and\n"
     ]
    }
   ],
   "source": [
    "# load document\n",
    "input_filename = 'junglebook.txt'\n",
    "doc = load_document(input_filename)\n",
    "print(doc[:2000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import string\n",
    " \n",
    "# turn a document into clean tokens\n",
    "def clean_document(doc):\n",
    "    doc = doc.replace('--', ' ')\n",
    "    tokens = doc.split()\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    tokens = [w.translate(table) for w in tokens]\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    tokens = [word.lower() for word in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['project', 'gutenberg', 'ebook', 'of', 'the', 'jungle', 'book', 'by', 'rudyard', 'kipling', 'this', 'ebook', 'is', 'for', 'the', 'use', 'of', 'anyone', 'anywhere', 'at', 'no', 'cost', 'and', 'with', 'almost', 'no', 'restrictions', 'whatsoever', 'you', 'may', 'copy', 'it', 'give', 'it', 'away', 'or', 'reuse', 'it', 'under', 'the', 'terms', 'of', 'the', 'project', 'gutenberg', 'license', 'included', 'with', 'this', 'ebook', 'or', 'online', 'at', 'wwwgutenbergorg', 'title', 'the', 'jungle', 'book', 'author', 'rudyard', 'kipling', 'release', 'date', 'january', 'ebook', 'last', 'updated', 'october', 'language', 'english', 'character', 'set', 'encoding', 'start', 'of', 'this', 'project', 'gutenberg', 'ebook', 'the', 'jungle', 'book', 'produced', 'by', 'an', 'anonymous', 'volunteer', 'and', 'david', 'widger', 'the', 'jungle', 'book', 'by', 'rudyard', 'kipling', 'contents', 'brothers', 'huntingsong', 'of', 'the', 'seeonee', 'pack', 'hunting', 'roadsong', 'of', 'the', 'bandarlog', 'song', 'the', 'white', 'seal', 'lukannon', 'chant', 'toomai', 'of', 'the', 'elephants', 'shiv', 'and', 'the', 'grasshopper', 'her', 'servants', 'parade', 'song', 'of', 'the', 'camp', 'animals', 'brothers', 'now', 'rann', 'the', 'kite', 'brings', 'home', 'the', 'night', 'that', 'mang', 'the', 'bat', 'sets', 'free', 'the', 'herds', 'are', 'shut', 'in', 'byre', 'and', 'hut', 'for', 'loosed', 'till', 'dawn', 'are', 'we', 'this', 'is', 'the', 'hour', 'of', 'pride', 'and', 'power', 'talon', 'and', 'tush', 'and', 'claw', 'oh', 'hear', 'the', 'call', 'good', 'hunting', 'all', 'that', 'keep', 'the', 'jungle', 'law', 'nightsong', 'in', 'the', 'jungle', 'it', 'was', 'seven', 'of', 'a', 'very', 'warm', 'evening', 'in', 'the', 'seeonee', 'hills']\n",
      "Total Tokens: 51473\n",
      "Unique Tokens: 5027\n"
     ]
    }
   ],
   "source": [
    "# clean document\n",
    "tokens = clean_document(doc)\n",
    "print(tokens[:200])\n",
    "print('Total Tokens: %d' % len(tokens))\n",
    "print('Unique Tokens: %d' % len(set(tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sequences: 51422\n"
     ]
    }
   ],
   "source": [
    "# organize into sequences (of length 50) of tokens\n",
    "length = 50 + 1\n",
    "sequences = list()\n",
    "for i in range(length, len(tokens)):\n",
    "    # select sequence of tokens\n",
    "    seq = tokens[i-length:i]\n",
    "    # convert into a line\n",
    "    line = ' '.join(seq)\n",
    "    sequences.append(line)\n",
    "print('Total Sequences: %d' % len(sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save tokens to file, one dialog per line\n",
    "def save_document(lines, name):\n",
    "    data = '\\n'.join(lines)\n",
    "    file = open(name, 'w')\n",
    "    file.write(data)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save sequences to file\n",
    "output_filename = 'junglebook_sequences.txt'\n",
    "save_document(sequences, output_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load document into memory\n",
    "def load_document(name):\n",
    "    file = open(name, 'r')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text\n",
    " \n",
    "# load\n",
    "input_filename = 'junglebook_sequences.txt'\n",
    "doc = load_document(input_filename)\n",
    "lines = doc.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# integer encode sequences of words\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(lines)\n",
    "sequences = tokenizer.texts_to_sequences(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# separate into input and output\n",
    "sequences = array(sequences)\n",
    "Input, Output = sequences[:,:-1], sequences[:,-1]\n",
    "Output = to_categorical(Output, num_classes=vocab_size)\n",
    "sequence_length = Input.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 50, 100)           502800    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 50, 200)           240800    \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 200)               320800    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 200)               40200     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 5028)              1010628   \n",
      "=================================================================\n",
      "Total params: 2,115,228\n",
      "Trainable params: 2,115,228\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "from keras.layers import Dropout\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 100, input_length=sequence_length))\n",
    "model.add(LSTM(200, return_sequences=True))\n",
    "model.add(LSTM(200))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(200, activation='relu'))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/75\n",
      "51422/51422 [==============================] - 517s 10ms/step - loss: 6.6069 - acc: 0.0682\n",
      "Epoch 2/75\n",
      "51422/51422 [==============================] - 501s 10ms/step - loss: 6.2250 - acc: 0.0721\n",
      "Epoch 3/75\n",
      "51422/51422 [==============================] - 494s 10ms/step - loss: 6.0805 - acc: 0.0827\n",
      "Epoch 4/75\n",
      "51422/51422 [==============================] - 453s 9ms/step - loss: 5.9354 - acc: 0.0911\n",
      "Epoch 5/75\n",
      "51422/51422 [==============================] - 451s 9ms/step - loss: 5.8014 - acc: 0.1025\n",
      "Epoch 6/75\n",
      "51422/51422 [==============================] - 448s 9ms/step - loss: 5.6800 - acc: 0.1126\n",
      "Epoch 7/75\n",
      "51422/51422 [==============================] - 448s 9ms/step - loss: 5.5646 - acc: 0.1198\n",
      "Epoch 8/75\n",
      "51422/51422 [==============================] - 447s 9ms/step - loss: 5.4614 - acc: 0.1267\n",
      "Epoch 9/75\n",
      "51422/51422 [==============================] - 447s 9ms/step - loss: 5.3677 - acc: 0.1315\n",
      "Epoch 10/75\n",
      "51422/51422 [==============================] - 449s 9ms/step - loss: 5.2885 - acc: 0.1342\n",
      "Epoch 11/75\n",
      "51422/51422 [==============================] - 450s 9ms/step - loss: 5.2218 - acc: 0.1380\n",
      "Epoch 12/75\n",
      "51422/51422 [==============================] - 449s 9ms/step - loss: 5.1429 - acc: 0.1402\n",
      "Epoch 13/75\n",
      "51422/51422 [==============================] - 449s 9ms/step - loss: 5.0917 - acc: 0.1424\n",
      "Epoch 14/75\n",
      "51422/51422 [==============================] - 448s 9ms/step - loss: 5.0171 - acc: 0.1452\n",
      "Epoch 15/75\n",
      "51422/51422 [==============================] - 448s 9ms/step - loss: 4.9520 - acc: 0.1473\n",
      "Epoch 16/75\n",
      "51422/51422 [==============================] - 448s 9ms/step - loss: 4.8880 - acc: 0.1506\n",
      "Epoch 17/75\n",
      "51422/51422 [==============================] - 447s 9ms/step - loss: 4.8307 - acc: 0.1551\n",
      "Epoch 18/75\n",
      "51422/51422 [==============================] - 448s 9ms/step - loss: 4.8129 - acc: 0.1550\n",
      "Epoch 19/75\n",
      "51422/51422 [==============================] - 450s 9ms/step - loss: 4.7857 - acc: 0.1548\n",
      "Epoch 20/75\n",
      "51422/51422 [==============================] - 449s 9ms/step - loss: 4.7032 - acc: 0.1593\n",
      "Epoch 21/75\n",
      "51422/51422 [==============================] - 450s 9ms/step - loss: 4.6548 - acc: 0.1600\n",
      "Epoch 22/75\n",
      "51422/51422 [==============================] - 448s 9ms/step - loss: 4.5812 - acc: 0.1629\n",
      "Epoch 23/75\n",
      "51422/51422 [==============================] - 447s 9ms/step - loss: 4.5474 - acc: 0.1641\n",
      "Epoch 24/75\n",
      "51422/51422 [==============================] - 448s 9ms/step - loss: 4.4725 - acc: 0.1664\n",
      "Epoch 25/75\n",
      "51422/51422 [==============================] - 447s 9ms/step - loss: 4.5027 - acc: 0.1659\n",
      "Epoch 26/75\n",
      "51422/51422 [==============================] - 449s 9ms/step - loss: 4.4486 - acc: 0.1674\n",
      "Epoch 27/75\n",
      "51422/51422 [==============================] - 448s 9ms/step - loss: 4.3099 - acc: 0.1745\n",
      "Epoch 28/75\n",
      "51422/51422 [==============================] - 452s 9ms/step - loss: 4.2418 - acc: 0.1782\n",
      "Epoch 29/75\n",
      "51422/51422 [==============================] - 462s 9ms/step - loss: 4.2303 - acc: 0.1788\n",
      "Epoch 30/75\n",
      "51422/51422 [==============================] - 448s 9ms/step - loss: 4.1416 - acc: 0.1838\n",
      "Epoch 31/75\n",
      "51422/51422 [==============================] - 450s 9ms/step - loss: 4.0701 - acc: 0.1886\n",
      "Epoch 32/75\n",
      "51422/51422 [==============================] - 448s 9ms/step - loss: 4.0057 - acc: 0.1921\n",
      "Epoch 33/75\n",
      "51422/51422 [==============================] - 448s 9ms/step - loss: 3.9404 - acc: 0.1977\n",
      "Epoch 34/75\n",
      "51422/51422 [==============================] - 448s 9ms/step - loss: 3.8961 - acc: 0.2004\n",
      "Epoch 35/75\n",
      "51422/51422 [==============================] - 450s 9ms/step - loss: 3.8313 - acc: 0.2064\n",
      "Epoch 36/75\n",
      "51422/51422 [==============================] - 449s 9ms/step - loss: 3.7746 - acc: 0.2139\n",
      "Epoch 37/75\n",
      "51422/51422 [==============================] - 450s 9ms/step - loss: 3.7493 - acc: 0.2157\n",
      "Epoch 38/75\n",
      "51422/51422 [==============================] - 449s 9ms/step - loss: 3.6876 - acc: 0.2225\n",
      "Epoch 39/75\n",
      "51422/51422 [==============================] - 447s 9ms/step - loss: 3.6356 - acc: 0.2274\n",
      "Epoch 40/75\n",
      "51422/51422 [==============================] - 451s 9ms/step - loss: 3.5717 - acc: 0.2344\n",
      "Epoch 41/75\n",
      "51422/51422 [==============================] - 447s 9ms/step - loss: 3.5353 - acc: 0.2374\n",
      "Epoch 42/75\n",
      "51422/51422 [==============================] - 448s 9ms/step - loss: 3.4846 - acc: 0.2462\n",
      "Epoch 43/75\n",
      "51422/51422 [==============================] - 448s 9ms/step - loss: 3.4388 - acc: 0.2502\n",
      "Epoch 44/75\n",
      "51422/51422 [==============================] - 455s 9ms/step - loss: 3.3920 - acc: 0.2545\n",
      "Epoch 45/75\n",
      "51422/51422 [==============================] - 453s 9ms/step - loss: 3.3505 - acc: 0.2589\n",
      "Epoch 46/75\n",
      "51422/51422 [==============================] - 448s 9ms/step - loss: 3.3113 - acc: 0.2662\n",
      "Epoch 47/75\n",
      "51422/51422 [==============================] - 448s 9ms/step - loss: 3.3232 - acc: 0.2664\n",
      "Epoch 48/75\n",
      "51422/51422 [==============================] - 450s 9ms/step - loss: 3.2610 - acc: 0.2730\n",
      "Epoch 49/75\n",
      "51422/51422 [==============================] - 450s 9ms/step - loss: 3.2505 - acc: 0.2748\n",
      "Epoch 50/75\n",
      "51422/51422 [==============================] - 450s 9ms/step - loss: 3.2856 - acc: 0.2771\n",
      "Epoch 51/75\n",
      "51422/51422 [==============================] - 450s 9ms/step - loss: 3.1842 - acc: 0.2860\n",
      "Epoch 52/75\n",
      "51422/51422 [==============================] - 449s 9ms/step - loss: 3.1172 - acc: 0.2948\n",
      "Epoch 53/75\n",
      "51422/51422 [==============================] - 455s 9ms/step - loss: 3.1662 - acc: 0.2918\n",
      "Epoch 54/75\n",
      "51422/51422 [==============================] - 454s 9ms/step - loss: 3.4129 - acc: 0.2656\n",
      "Epoch 55/75\n",
      "51422/51422 [==============================] - 449s 9ms/step - loss: 3.3144 - acc: 0.2733\n",
      "Epoch 56/75\n",
      "51422/51422 [==============================] - 551s 11ms/step - loss: 3.2530 - acc: 0.2807\n",
      "Epoch 57/75\n",
      "51422/51422 [==============================] - 539s 10ms/step - loss: 3.1926 - acc: 0.2868\n",
      "Epoch 58/75\n",
      "51422/51422 [==============================] - 532s 10ms/step - loss: 3.1441 - acc: 0.2928\n",
      "Epoch 59/75\n",
      "51422/51422 [==============================] - 529s 10ms/step - loss: 3.0970 - acc: 0.2979\n",
      "Epoch 60/75\n",
      "51422/51422 [==============================] - 541s 11ms/step - loss: 3.0582 - acc: 0.3036\n",
      "Epoch 61/75\n",
      "51422/51422 [==============================] - 524s 10ms/step - loss: 3.0121 - acc: 0.3111\n",
      "Epoch 62/75\n",
      "51422/51422 [==============================] - 530s 10ms/step - loss: 2.9672 - acc: 0.3175\n",
      "Epoch 63/75\n",
      "51422/51422 [==============================] - 532s 10ms/step - loss: 2.9369 - acc: 0.3231\n",
      "Epoch 64/75\n",
      "51422/51422 [==============================] - 544s 11ms/step - loss: 2.8845 - acc: 0.3300\n",
      "Epoch 65/75\n",
      "51422/51422 [==============================] - 579s 11ms/step - loss: 2.8595 - acc: 0.3357\n",
      "Epoch 66/75\n",
      "51422/51422 [==============================] - 525s 10ms/step - loss: 2.8161 - acc: 0.3400\n",
      "Epoch 67/75\n",
      "51422/51422 [==============================] - 458s 9ms/step - loss: 2.7810 - acc: 0.3441\n",
      "Epoch 68/75\n",
      "51422/51422 [==============================] - 516s 10ms/step - loss: 2.7346 - acc: 0.3547\n",
      "Epoch 69/75\n",
      "51422/51422 [==============================] - 522s 10ms/step - loss: 2.7065 - acc: 0.3570\n",
      "Epoch 70/75\n",
      "51422/51422 [==============================] - 458s 9ms/step - loss: 2.6710 - acc: 0.3642\n",
      "Epoch 71/75\n",
      "51422/51422 [==============================] - 449s 9ms/step - loss: 2.6264 - acc: 0.3716\n",
      "Epoch 72/75\n",
      "51422/51422 [==============================] - 450s 9ms/step - loss: 2.6027 - acc: 0.3766\n",
      "Epoch 73/75\n",
      "51422/51422 [==============================] - 461s 9ms/step - loss: 2.5761 - acc: 0.3784\n",
      "Epoch 74/75\n",
      "51422/51422 [==============================] - 454s 9ms/step - loss: 2.5370 - acc: 0.3874\n",
      "Epoch 75/75\n",
      "51422/51422 [==============================] - 450s 9ms/step - loss: 2.5038 - acc: 0.3938\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1131338d0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# fit model\n",
    "model.fit(Input, Output, batch_size=250, epochs=75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save the model to file\n",
    "model.save('junglebook_trained.h5')\n",
    "# save the tokenizer\n",
    "dump(tokenizer, open('tokenizer.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load doc into memory\n",
    "def load_document(name):\n",
    "    file = open(name, 'r')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text\n",
    " \n",
    "# load cleaned text sequences\n",
    "input_filename = 'junglebook_sequences.txt'\n",
    "doc = load_document(input_filename)\n",
    "lines = doc.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sequence_length = len(lines[0].split()) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model\n",
    "from keras.models import load_model\n",
    "model = load_model('junglebook_trained.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "to me not long ago with some rude talk that i was a naked cub and not fit to dig pignuts but i caught tabaqui by the tail and swung him twice against a palmtree to teach him better was foolishness for though tabaqui is a mischiefmaker he would have told\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# select a seed text\n",
    "from random import randint\n",
    "seed_text = lines[randint(0,len(lines))]\n",
    "print(seed_text + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoded = tokenizer.texts_to_sequences([seed_text])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from random import randint\n",
    "from pickle import load\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    " \n",
    "# load doc into memory\n",
    "def load_document(name):\n",
    "    file = open(name, 'r')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text\n",
    " \n",
    "# generate a sequence from a language model\n",
    "def generate_sequence(model, tokenizer, sequence_length, seed_text, n_words):\n",
    "\tresult = list()\n",
    "\tinput_text = seed_text\n",
    "\t# generate a fixed number of words\n",
    "\tfor _ in range(n_words):\n",
    "\t\t# encode the text as integer\n",
    "\t\tencoded = tokenizer.texts_to_sequences([input_text])[0]\n",
    "\t\t# truncate sequences to a fixed length\n",
    "\t\tencoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
    "\t\t# predict probabilities for each word\n",
    "\t\tprediction = model.predict_classes(encoded, verbose=0)\n",
    "\t\t# map predicted word index to word\n",
    "\t\tout_word = ''\n",
    "\t\tfor word, index in tokenizer.word_index.items():\n",
    "\t\t\tif index == prediction:\n",
    "\t\t\t\tout_word = word\n",
    "\t\t\t\tbreak\n",
    "\t\t# append to input\n",
    "\t\tinput_text += ' ' + out_word\n",
    "\t\tresult.append(out_word)\n",
    "\treturn ' '.join(result)\n",
    " \n",
    "# load cleaned text sequences\n",
    "input_filename = 'junglebook_sequences.txt'\n",
    "doc = load_document(input_filename)\n",
    "lines = doc.split('\\n')\n",
    "seq_length = len(lines[0].split()) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baskets of dried grass and put grasshoppers in them or catch two praying mantises and make them fight or string a necklace of red and black jungle nuts or watch a lizard basking on a rock or a snake hunting a frog near the wallows then they sing long long songs\n",
      "\n",
      "with odd native quavers at the end of the review and the hyaena whom he had seen the truth they feel twitched to the noises round him for a picture of the end of the ravine and snuffing bitten and best of the bulls at the dawn is a native\n"
     ]
    }
   ],
   "source": [
    "# load the model\n",
    "model = load_model('junglebook_trained.h5')\n",
    " \n",
    "# load the tokenizer\n",
    "tokenizer = load(open('tokenizer.pkl', 'rb'))\n",
    " \n",
    "# select a seed text\n",
    "seed_text = lines[randint(0,len(lines))]\n",
    "print(seed_text + '\\n')\n",
    " \n",
    "# generate new text\n",
    "generated = generate_sequence(model, tokenizer, sequence_length, seed_text, 50)\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "little toomai there was a splash and a trample and the rush of running water and kala nag strode through the bed of a river feeling his way at each step above the noise of the water as it swirled round the legs little toomai could hear more splashing and some\n",
      "\n",
      "trumpeting both upstream and down grass and knocked him up to the jealous moon he could see bruised of dust for the potter was rann caught him up to the plowed din of the melbourne lines where the two wolves would be forced to make themselves rifles and the sparks\n"
     ]
    }
   ],
   "source": [
    "# load the model\n",
    "model = load_model('junglebook_trained.h5')\n",
    " \n",
    "# load the tokenizer\n",
    "tokenizer = load(open('tokenizer.pkl', 'rb'))\n",
    " \n",
    "# select a seed text\n",
    "seed_text = lines[randint(0,len(lines))]\n",
    "print(seed_text + '\\n')\n",
    " \n",
    "# generate new text\n",
    "generated = generate_sequence(model, tokenizer, sequence_length, seed_text, 50)\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is in their legs and he remembered the good firm beaches of novastoshnah seven thousand miles away the games his companions played the smell of the seaweed the seal roar and the fighting that very minute he turned north swimming steadily and as he went on he met scores of his\n",
      "\n",
      "mates and bound like the deck of the fighters and harness under his breath and he could not be able to stop a ship and ducked to nag wound up with scores of marble tracery showing all the regiments went twisting his head and shoulders and creepers very seldom shows\n"
     ]
    }
   ],
   "source": [
    "# load the model\n",
    "model = load_model('junglebook_trained.h5')\n",
    " \n",
    "# load the tokenizer\n",
    "tokenizer = load(open('tokenizer.pkl', 'rb'))\n",
    " \n",
    "# select a seed text\n",
    "seed_text = lines[randint(0,len(lines))]\n",
    "print(seed_text + '\\n')\n",
    " \n",
    "# generate new text\n",
    "generated = generate_sequence(model, tokenizer, sequence_length, seed_text, 50)\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
